{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcseYrypcR6DndU1nHIDwv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomascerbelaud/M2-IPE-data-energy/blob/main/notebooks/4_1_Bias_Variance_Tradeoff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 4.1 : Bias-variance tradeoff\n",
        "\n",
        "The goal of this notebook is to highlight the bias-variance tradeoff in a simple model, and plot the bias and variance curves with respect to model"
      ],
      "metadata": {
        "id": "0Q0KgaqgX-hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing on a simple function\n",
        "\n",
        "Consider the following function:\n",
        "\n",
        "$$ f(x) = 1 - x^2 - 2 e^{-100x^2} $$\n",
        "\n",
        "Fit a polynomial regressor to estimate noisy measurements coming from $f$, with growing complexity. The higher the polynomial degree, and the better the fitted curve pass through the training data points, at the price of stability and generalization."
      ],
      "metadata": {
        "id": "YUB69FD3RheB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "def f(x: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"True generative process of the data that we are trying to learn.\"\"\"\n",
        "  return 1 - x ** 2 - 2 * np.exp(-100 * x ** 2)\n",
        "  # return x ** 2\n",
        "\n",
        "\n",
        "def generate_dataset(num_inputs: int, noise_std: float = 0.0, a_min: float = -1.0, a_max: float = 1.0) -> tuple[np.ndarray, np.ndarray]:\n",
        "  \"\"\"Generate a random (X, y) pair of data.\"\"\"\n",
        "  x = np.random.uniform(a_min, a_max, size=(num_inputs, 1))\n",
        "  y = f(x) + np.random.normal(scale=noise_std, size=(num_inputs, 1))\n",
        "  return x, y\n",
        "\n",
        "\n",
        "def fit_polynomial_model(x: np.ndarray, y: np.ndarray, degree: int = 1) -> LinearRegression:\n",
        "  lr = Pipeline([\n",
        "      (\"polynomial_features\", PolynomialFeatures(degree=degree, include_bias=False)),\n",
        "      (\"linear_model\", LinearRegression())\n",
        "  ]).fit(x, y.ravel())\n",
        "  return lr\n"
      ],
      "metadata": {
        "id": "761MVG-5Rgu9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill in the code after each `TODO` blocks in the function `plot_data_and_regressor`. Test running it for different maximum polynomial degrees. What do you observe?"
      ],
      "metadata": {
        "id": "OHC9le2-wvKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data_and_regressor(x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray, degree: int) -> None:\n",
        "  x = np.linspace(-1, 1, num=1000)\n",
        "\n",
        "  # plot true curve\n",
        "  plt.plot(x, f(x), \"-\")\n",
        "\n",
        "  # TODO: plot data points\n",
        "\n",
        "  # TODO: fit polynomial regression lr\n",
        "  lr = None\n",
        "\n",
        "  # Plot fitted curve\n",
        "  plt.plot(x, lr.predict(x.reshape(-1, 1)).ravel())\n",
        "  plt.xlabel(\"$x$\")\n",
        "  plt.ylabel(\"$y$\")\n",
        "  plt.legend([\n",
        "      \"$f(x)$\",\n",
        "      \"Dataset with noise\",\n",
        "      \"Fitted curve\"\n",
        "  ])\n",
        "\n",
        "  # TODO: print MSE\n",
        "  mse = 0.0\n",
        "  print(f\"MSE: {mse:.3f}\")\n",
        "\n",
        "\n",
        "x_train, y_train = generate_dataset(num_inputs=50, noise_std=0.1)\n",
        "x_test, y_test = generate_dataset(num_inputs=20, noise_std=0.1)\n",
        "degree = 5\n",
        "plot_data_and_regressor(x_train, y_train, x_test, y_test, degree=degree)"
      ],
      "metadata": {
        "id": "sTrwIodXRgM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting bias-variance tradeoff\n",
        "\n",
        "Let us study a linear regression model. Let $X' = (x_1, .., x_n) \\in \\mathbb{R}^{n \\times d}$, and $y = (y_1, .., y_n) \\in \\mathbb{R}^n$ be the noisy observations s.a.:\n",
        "\n",
        "$$ y = f(x) + ϵ = X' \\beta^T + ϵ, \\quad ϵ \\sim \\mathcal{N}(0, σ) $$\n",
        "\n",
        "We will have at our disposal a matrix of inputs which is the concatenation of $X'$ with another feature matrix $Z$ that was not use to generate the observations $y$, $X = (X', Z) \\in \\mathbb{R}^{n \\times q}$ with $q > d$. The objective of this section is to train a linear model of growing complexity $p$, i.e. we add complexity by adding new variables, in order, from the columns of $X$. We expect the generalization error to improve up to complexity $p = d$, an then to decrease as we add dummy variables to the training set."
      ],
      "metadata": {
        "id": "hcPokrtmUfV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBSqomOHX9FU"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "d = 100  # number of features\n",
        "coefs = np.array([87.0580698 , 86.04083468, 72.64348442, 45.45118925, 41.77504125, 31.0259999 , 26.09989212, 16.60838184, 11.64048518,  8.57020267])\n",
        "coefs = np.concatenate([coefs, np.zeros(d - len(coefs))])\n",
        "\n",
        "\n",
        "def f(x: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"True generative process of the data that we are trying to learn.\"\"\"\n",
        "  return x @ coefs\n",
        "\n",
        "\n",
        "def generate_dataset(num_inputs: int, noise_std: float = 0.0, a_min: float = -1.0, a_max: float = 1.0) -> tuple[np.ndarray, np.ndarray]:\n",
        "  \"\"\"Generate a random (X, y) pair of data.\"\"\"\n",
        "  x = np.random.uniform(-1.0, 1.0, size=(num_inputs, d))\n",
        "  y = f(x) + np.random.normal(scale=noise_std, size=num_inputs)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "def fit_linear_model(x: np.ndarray, y: np.ndarray, k_best: int = 1) -> LinearRegression:\n",
        "  lr = LinearRegression(fit_intercept=False).fit(x[:, :k_best], y.ravel())\n",
        "  return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember the following formulae for the bias and variance of the estimator $\\hat{f}(X) = X \\beta^T$, given that we know the true generative function $f$, at input $x_0$. The expectation is computed over random training sets $\\mathcal{T}$ generated using the `generate_dataset` function, for a number of rounds $R$.\n",
        "\n",
        "\\begin{align}\n",
        "  \\textrm{MSE}(x_0) &= \\textrm{Bias}^2(\\hat{f}; x_0) + \\textrm{Var}(\\hat{f}; x_0) \\\\\n",
        "  \\textrm{Bias}(\\hat{f}; x_0) &= f(x) - \\mathbb{E}_\\mathcal{T}[\\hat{f}(x_0)] \\\\\n",
        "  &= f(x_0) - \\dfrac{1}{R} \\sum_{i=1}^R \\hat{f}(x_0) \\\\\n",
        "  \\textrm{Var}(\\hat{f}; x_0) &= \\mathbb{E}_\\mathcal{T}\\Big[\\Big(\\hat{f}(x_0) - \\mathbb{E}_\\mathcal{T}[\\hat{f}(x_0)] \\Big)^2 \\Big] \\\\\n",
        "  &= \\dfrac{1}{R} \\sum_{i=1}^R\\Big(\\hat{f}(x_0) - \\dfrac{1}{R} \\sum_{i=1}^R \\hat{f}(x_0)\\Big)^2\n",
        "\\end{align}\n",
        "\n",
        "Now, given a test set $\\mathcal{T}'$, we can marginalise the estimated bias (squared) and variance over $x_0$ to estimate the squared bias and variance of the estimator, and discretizing the integral over $\\mathcal{T}'$. Remember that our goal is to estimate the term in the bias-variance decomposition, which is given at $x$, but that we want to marginalize / average to study our estimator generalization ability.\n",
        "\n",
        "\\begin{align}\n",
        "  \\textrm{Bias}^2(\\hat{f}) &= \\dfrac{1}{|\\mathcal{T}|} \\sum_{x_0 \\in \\mathcal{T}'} \\textrm{Bias}^2(\\hat{f}; x_0) \\\\\n",
        "  \\textrm{Var}(\\hat{f}) &= \\dfrac{1}{|\\mathcal{T}|} \\sum_{x_0 \\in \\mathcal{T}'} \\textrm{Var}(\\hat{f}; x_0)\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "YF18SJZBjYM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_squared_bias(f: np.ndarray, f_hat: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Compute the bias given ground truth and predictions coming from different estimators\n",
        "  trained on random datasets R times.\n",
        "\n",
        "  Shapes:\n",
        "    N: number of input points in the evaluation / test set\n",
        "    P: maximum complexity. Also corresponds to the number of models with growing complexities that are trained\n",
        "    f: (N,)\n",
        "    f_hat: (R, P, N)\n",
        "\n",
        "  Returns:\n",
        "    Bias, array of shape (P,)\n",
        "  \"\"\"\n",
        "  # TODO\n",
        "\n",
        "\n",
        "def compute_variance(f_hat: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Compute the bias given predictions coming from different estimators trained on random datasets R times.\n",
        "\n",
        "  Shapes:\n",
        "    N: number of input points in the evaluation / test set\n",
        "    P: maximum complexity. Also corresponds to the number of models with growing complexities that are trained\n",
        "    f_hat: (R, P, N)\n",
        "\n",
        "  Returns:\n",
        "    Variance, array of shape (P,)\n",
        "  \"\"\"\n",
        "  # TODO\n",
        "\n",
        "\n",
        "def compute_mean_squared_error(y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"Compute the mean sum of squared errors between predictions and a ground truth, over\n",
        "  the observations from a test set.\n",
        "\n",
        "  Shapes:\n",
        "    N: number of input points in the evaluation / test set\n",
        "    P: maximum complexity. Also corresponds to the number of models with growing complexities that are trained\n",
        "    y_true: (1 or R, 1, N)\n",
        "    y_pred: (R, P, N)\n",
        "\n",
        "  Notes:\n",
        "    y_true and y_pred naturally broadcast each other, meaning that y_true + y_pred\n",
        "    results in the element-wise sum across each dimension.\n",
        "  \"\"\"\n",
        "  # TODO\n",
        "\n",
        "\n",
        "def bias_variance_decomposition(num_rounds: int = 100, max_k: int = 30, num_train: int = 80, num_test: int = 20, noise_std=0.1) -> None:\n",
        "  \"\"\"Decompose bias and variance of the MSE error as a function of the maximum polynomial degree of the estimator.\"\"\"\n",
        "\n",
        "  # Test data, and placeholders\n",
        "  x_test, y_test = generate_dataset(num_inputs=num_test, noise_std=noise_std)\n",
        "  f_test = np.empty((num_rounds, max_k, num_test))\n",
        "  f_train = np.empty((num_rounds, max_k, num_train))\n",
        "  y_train = np.empty((num_rounds, 1, num_train))\n",
        "\n",
        "  with tqdm.trange(num_rounds * max_k) as pbar:\n",
        "    for n in range(num_rounds):\n",
        "      # TODO: generate dataset\n",
        "      x, y = None, None\n",
        "      y_train[n, 0] = y.ravel()\n",
        "\n",
        "      # Fit model of growing complexity\n",
        "      for k in range(1, max_k + 1):\n",
        "        # TODO: fit linear regression\n",
        "        lr = None\n",
        "\n",
        "        f_test[n, k - 1] = lr.predict(x_test[:, :k]).ravel()\n",
        "        f_train[n, k - 1] = lr.predict(x[:, :k]).ravel()\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    # compute bias and variances\n",
        "    squared_bias = compute_squared_bias(f(x_test), f_test)\n",
        "    variance = compute_variance(f_test)\n",
        "\n",
        "    # compute MSE for train and test sets\n",
        "    mse_test = compute_mean_squared_error(y_test.ravel(), f_test)\n",
        "    mse_train = compute_mean_squared_error(y_train, f_train)\n",
        "\n",
        "    # plot results\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    ax1.plot(squared_bias, c=\"r\")\n",
        "    ax1.plot(variance, c=\"b\")\n",
        "    ax1.legend([\"Bias\", \"Variance\"])\n",
        "    plt.xlabel(\"Model complexity, $p$\")\n",
        "    ax1.set_ylabel(\"Bias / Variance\")\n",
        "    ax2.set_xlabel(\"Model complexity, $p$\")\n",
        "\n",
        "    ax2.plot(mse_test.mean(axis=0), c=\"r\")\n",
        "    ax2.plot(mse_train.mean(axis=0), c=\"b\")\n",
        "    ax2.plot(mse_test.T, c=\"r\", alpha=.05)\n",
        "    ax2.plot(mse_train.T, c=\"b\", alpha=.05)\n",
        "    ax2.legend([\"Test\", \"Train\"])\n",
        "    ax2.set_xlabel(\"Model complexity, $p$\")\n",
        "    ax2.set_ylabel(\"MSE\")\n",
        "\n",
        "bias_variance_decomposition(num_train=50, num_test=20, num_rounds=100, max_k=35, noise_std=30.0)"
      ],
      "metadata": {
        "id": "c25lSHzPdNlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QgVLsjXjseos"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}