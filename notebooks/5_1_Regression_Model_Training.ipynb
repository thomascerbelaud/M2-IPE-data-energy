{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWyuOvM4EOOmVp3HQPF9tN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomascerbelaud/M2-IPE-data-energy/blob/main/notebooks/5_1_Regression_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 5.1: Regression model training"
      ],
      "metadata": {
        "id": "Z5Y2wxxy0BO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting building energy loads\n",
        "\n",
        "### Dataset Information\n",
        "\n",
        "This dataset was crafted using building energy simulations of a very simple building, with different parameters described by parameters $X_1, .., X_8$. For each simulation, the total cooling and heating loads were reported (no units available / found online...).\n",
        "\n",
        "> \"We perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. We simulate various settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 features, aiming to predict two real valued responses. It can also be used as a multi-class classification problem if the response is rounded to the nearest integer.\"\n",
        "\n",
        "### Variable description\n",
        "\n",
        "| Variable | Name | Role | Type | Description | Units | Missing Values |\n",
        "| :------- | :--- | :--- | :--- | :---------- | :---- | :------------- |\n",
        "| X1       | Feature | Continuous | Relative Compactness |  %   |       | no             |\n",
        "| X2       | Feature | Continuous | Surface Area |   m²  |       | no             |\n",
        "| X3       | Feature | Continuous | Wall Area |   m²  |       | no             |\n",
        "| X4       | Feature | Continuous | Roof Area |  m²   |       | no             |\n",
        "| X5       | Feature | Continuous | Overall Height |   m  |       | no             |\n",
        "| X6       | Feature | Integer | Orientation |  2:North, 3:East, 4:South, 5:West   |       | no             |\n",
        "| X7       | Feature | Continuous | Glazing Area |  0%, 10%, 25%, 40% (of floor area)   |       | no             |\n",
        "| X8       | Feature | Integer | Glazing Area Distribution |   1:Uniform, 2:North, 3:East, 4:South, 5:West  |       | no             |\n",
        "| Y1       | Target | Continuous | Heating Load |  kWh / m²   |       | no             |\n",
        "| Y2       | Target | Continuous | Cooling Load |  kWh / m²   |       | no             |\n",
        "\n",
        "> *Accurate quantitative estimation of energy performance of residential buildings using statistical machine learning tools*\n",
        "By A. Tsanas, Angeliki Xifara. 2012, Published in Energy and Buildings, vol. 49"
      ],
      "metadata": {
        "id": "stvojABkm3ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "oclHNOObXYlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "\n",
        "def load_file_from_url(url: str) -> pd.DataFrame:\n",
        "  with tempfile.TemporaryDirectory() as tempdir:\n",
        "    # download\n",
        "    zip_path = os.path.join(tempdir, \"file.zip\")\n",
        "    urlretrieve(url, zip_path)\n",
        "\n",
        "    # unpack and delete source\n",
        "    shutil.unpack_archive(zip_path, tempdir)\n",
        "    os.remove(zip_path)\n",
        "\n",
        "    # load file\n",
        "    filename = glob.glob(f\"{tempdir}/*.xlsx\")[0]\n",
        "    df = pd.read_excel(filename)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "# df = load_files_from_url_list(urls)\n",
        "df = load_file_from_url(\"https://archive.ics.uci.edu/static/public/242/energy+efficiency.zip\")"
      ],
      "metadata": {
        "id": "P6tm3T6T4eRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns, df.shape"
      ],
      "metadata": {
        "id": "6G9UDVsd42xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment to visualize the dataset distribution."
      ],
      "metadata": {
        "id": "KZlBid_SXcyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import seaborn as sns\n",
        "\n",
        "# sns.pairplot(df);"
      ],
      "metadata": {
        "id": "BT-CEux243jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train predictors\n",
        "\n",
        "We are going to train three types of regressors:\n",
        "\n",
        "* Linear regressor\n",
        "* Random forest regressor\n",
        "* Gradient boosting machine regressor\n",
        "\n",
        "Regarding the linear models, we will describe a way of treating categorical variables in regression called *one hot encoding*. It consists in transforming a column of $K$ categories into a matrix with $K$ columns \"one hot encoded\", as described below. To make this transformation, pandas `pd.get_dummies` method can be used, as well as scikit learn preprocessing class `sklearn.preprocessing.OneHotEncoder`.\n",
        "\n",
        "![https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T5jaa2othYfXZX9W.](https://miro.medium.com/v2/resize:fit:1400/format:webp/0*T5jaa2othYfXZX9W.)\n",
        "\n",
        "See [scikit-learn user guide](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features) for more information."
      ],
      "metadata": {
        "id": "JzphEGcbXkg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: split the data frame into the continuous, categorical and response variables\n",
        "X_continuous = None\n",
        "X_categorical = None\n",
        "y = None"
      ],
      "metadata": {
        "id": "3oIXiYtB8vjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into train and test set\n",
        "(\n",
        "    X_train_continuous,\n",
        "    X_test_continuous,\n",
        "    X_train_categorical,\n",
        "    X_test_categorical,\n",
        "    y_train,\n",
        "    y_test,\n",
        ") = train_test_split(X_continuous, X_categorical, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# scale continuous data\n",
        "scaler = StandardScaler().fit(X_train_continuous)\n",
        "X_train_continuous_scaled = scaler.transform(X_train_continuous)\n",
        "# TODO: scale X_test_continuous\n",
        "X_test_continuous_scaled = X_test_continuous\n",
        "\n",
        "# one hot encoding of categorical variables\n",
        "categorical_columns = X_train_categorical.columns\n",
        "X_train_categorical_one_hot = pd.get_dummies(X_train_categorical, columns=categorical_columns)\n",
        "# TODO: one hot X_test_categorical\n",
        "X_test_categorical_one_hot = X_test_categorical\n",
        "\n",
        "# dataset without one hot encoding\n",
        "X_train = np.concatenate([X_train_continuous, X_train_categorical], axis=1)\n",
        "X_test = np.concatenate([X_test_continuous, X_test_categorical], axis=1)\n",
        "\n",
        "# dataset with one hot encoding\n",
        "X_train_with_one_hot = np.concatenate([X_train_continuous_scaled, X_train_categorical_one_hot], axis=1)\n",
        "X_test_with_one_hot = np.concatenate([X_test_continuous_scaled, X_test_categorical_one_hot], axis=1)\n"
      ],
      "metadata": {
        "id": "Y2a4lmyT8zXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Visualize correlations using PCA\n",
        "# TODO: train a PCA model on the continuous variables with 6 principal components\n",
        "pca = None\n",
        "print(f\"\"\"\n",
        "Number of principal components: {pca.n_components_}\n",
        "Cumulative explained variance ratio: {pca.explained_variance_ratio_.sum():.2f}\"\"\")\n",
        "\n",
        "# This snippet plot the principal components coordinates. What can you say about\n",
        "# these principal components and the directions of highest variances?\n",
        "variables_mapping = {\n",
        "    \"X1\": \"Relative Compactness\",\n",
        "    \"X2\": \"Surface Area\",\n",
        "    \"X3\": \"Wall Area\",\n",
        "    \"X4\": \"Roof Area\",\n",
        "    \"X5\": \"Overall Height\",\n",
        "    \"X6\": \"Orientation\",\n",
        "    \"X7\": \"Glazing Area\",\n",
        "    \"X8\": \"Glazing Area Distribution\",\n",
        "}\n",
        "variables = [variables_mapping[col] for col in X_continuous.columns]\n",
        "sns.heatmap(pca.components_.T, cmap=\"viridis\", yticklabels=variables, xticklabels=[f\"PC$_{{{i + 1}}}$\" for i in range(pca.n_components_)])"
      ],
      "metadata": {
        "id": "LfHqPXcBBQ0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# linear model on both datasets\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "\n",
        "# TODO: train a linear regression without preprocessing the categorical features\n",
        "# without one hot\n",
        "lr1 = None\n",
        "y_pred = lr1.predict(X_test)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred, multioutput=\"raw_values\") * 100\n",
        "print(f\"MAPE without one hot: heating_load={mape[0]:.2f}%, cooling_load={mape[1]:.2f}%\")\n",
        "\n",
        "# TODO: train a linear regressor using the one hot encoded categorical features\n",
        "# with one hot\n",
        "lr2 = None\n",
        "y_pred = lr2.predict(X_test_with_one_hot)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred, multioutput=\"raw_values\") * 100\n",
        "print(f\"MAPE with one hot: heating_load={mape[0]:.2f}%, cooling_load={mape[1]:.2f}%\")\n",
        "\n",
        "# TODO: train a random forest regressors\n",
        "# with RF\n",
        "rf = None\n",
        "y_pred = rf.predict(X_test)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred, multioutput=\"raw_values\") * 100\n",
        "print(f\"MAPE with RF: heating_load={mape[0]:.2f}%, cooling_load={mape[1]:.2f}%\")\n",
        "\n",
        "# TODO: train a gradient boosting regressor. BE CAREFUL: scikit-learn class cannot\n",
        "# regress on multiple outputs!\n",
        "# - train gbm1 on the first output (heating load)\n",
        "# - train gbm2 on the second output (cooling load)\n",
        "# with GBM\n",
        "gbm1 = None\n",
        "gbm2 = None\n",
        "y_pred1 = gbm1.predict(X_test)\n",
        "y_pred2 = gbm2.predict(X_test)\n",
        "y_pred = np.stack([y_pred1, y_pred2], axis=1)\n",
        "mape = mean_absolute_percentage_error(y_test, y_pred, multioutput=\"raw_values\") * 100\n",
        "print(f\"MAPE with GBM: heating_load={mape[0]:.2f}%, cooling_load={mape[1]:.2f}%\")\n"
      ],
      "metadata": {
        "id": "IRe6S0JhBnOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plot feature importance. What do you see?\n",
        "# use rf.feature_importances_"
      ],
      "metadata": {
        "id": "XCoDtE8zBsgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}