{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomascerbelaud/M2-IPE-data-energy/blob/main/notebooks/5_2_Principal_Component_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 5.1: Principal Component Analysis\n",
        "\n",
        "This notebook covers various cases when principal component analysis can be useful in machine learning. For instance, it can be used for data compression, visualisation, and feature extraction."
      ],
      "metadata": {
        "id": "ywfjlhXx_p4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using PCA for visualization and clustering\n",
        "\n",
        "PCA can be a powerful tool to extract visualize high-dimensional data distribution in 2D or 3D. Furthermore, the learned features (a linear combination of the original features) are often more expressive and robust for downstream tasks like clustering, regression or classification.\n",
        "\n",
        "In this section we demonstrate its use on the IRIS dataset. It consists of 150 samples of iris flowers, with 50 samples from each of three species: Setosa, Versicolour, and Virginica. Each sample has four features: sepal length, sepal width, petal length, and petal width, stored in a 150x4 numpy.ndarray where the rows represent the samples and the columns represent the features. The dataset is available through the `load_iris()` function in scikit-learn, which returns the data and target variables, with the target being an array of integers (0 for Setosa, 1 for Versicolour, and 2 for Virginica) corresponding to the species."
      ],
      "metadata": {
        "id": "gK92wyyJmIOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "Ye6oowsxen0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "bunch = load_iris()\n",
        "X = bunch.data\n",
        "y = bunch.target\n",
        "\n",
        "# print(bunch.DESCR)\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.DataFrame(np.concatenate([X, y.reshape(-1, 1)], axis=1))\n",
        "data.columns = bunch.feature_names + [\"labels\"]\n",
        "g = sns.pairplot(data, hue=\"labels\")\n",
        "legend = g.legend\n",
        "for text, label in zip(legend.get_texts(), bunch.target_names):\n",
        "    text.set_text(label)"
      ],
      "metadata": {
        "id": "qR79yOLqTGDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment to display dataset description\n",
        "# print(bunch.DESCR)"
      ],
      "metadata": {
        "id": "BaNqBE-uefhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA"
      ],
      "metadata": {
        "id": "PwzaklkwespT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# TODO: project the IRIS dataset into a 2D basis of the first principal components, and visualize it.\n",
        "# What do you see?\n",
        "pca = None\n",
        "X_reduced = None\n",
        "pc_1, pc_2 = X_reduced.T"
      ],
      "metadata": {
        "id": "xA4locWpS5nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Face recognition\n",
        "\n",
        "PCA can be applied to images as well, either for compression or feature extraction. We will use a dataset made of the faces of member of various governments. The goal is then, given a new face, to predict whose face is it.\n",
        "\n",
        "*N.B.: this is largely inspired from [scikit-learn example](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html).*"
      ],
      "metadata": {
        "id": "XUJzlII_mxM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "cKCbkq1qfJ3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "\n",
        "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
        "\n",
        "# introspect the images arrays to find the shapes (for plotting)\n",
        "n_samples, h, w = lfw_people.images.shape\n",
        "\n",
        "# for machine learning we use the 2 data directly (as relative pixel\n",
        "# positions info is ignored by this model)\n",
        "X = lfw_people.data\n",
        "n_features = X.shape[1]\n",
        "\n",
        "# the label to predict is the id of the person\n",
        "y = lfw_people.target\n",
        "target_names = lfw_people.target_names\n",
        "n_classes = target_names.shape[0]\n",
        "\n",
        "print(\"Total dataset size:\")\n",
        "print(\"n_samples: %d\" % n_samples)\n",
        "print(\"n_features: %d\" % n_features)\n",
        "print(\"n_classes: %d\" % n_classes)\n",
        "print(\"picture dimensions: %d x %d\" % (h, w))"
      ],
      "metadata": {
        "id": "GFphC0E-mwqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n",
        "    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
        "    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
        "    plt.subplots_adjust(bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n",
        "    for i in range(n_row * n_col):\n",
        "        plt.subplot(n_row, n_col, i + 1)\n",
        "        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
        "        plt.title(titles[i], size=10)\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "\n",
        "\n",
        "titles = [target_names[label] for label in y[:12]]\n",
        "images = X[:12]\n",
        "plot_gallery(images, titles, h, w)\n",
        "plt.suptitle(\"First 12 faces of the dataset\");"
      ],
      "metadata": {
        "id": "g-5Ajy8LcHtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split and scale dataset"
      ],
      "metadata": {
        "id": "hcR5vRiGfNIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: split data set into train and test set, and scale the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_train, X_test, y_train, y_test = (None, None, None, None)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = None\n",
        "X_test = None"
      ],
      "metadata": {
        "id": "2sijJ5WabhZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Project the images into a basis of dimension 150 using PCA.\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "8-94axBJ2hqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code below displays the first 12 principal components. You can think of the\n",
        "# principal components as features that are crafted from the raw images pixels.\n",
        "n_components_to_display = 12\n",
        "titles = [f\"PC$_{{{i + 1}}}$\" for i in range(n_components_to_display)]\n",
        "plot_gallery(pca.components_[:n_components_to_display], titles, h, w)\n",
        "plt.suptitle(f\"First {n_components_to_display} principal components\");"
      ],
      "metadata": {
        "id": "96yGfG682hMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
        "\n",
        "\n",
        "def classification_report_and_plot(y_test: np.ndarray, X_test: np.ndarray, target_names: list[str], model: BaseEstimator):\n",
        "    \"\"\"Make classification report and plots.\"\"\"\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "    ConfusionMatrixDisplay.from_estimator(\n",
        "        model, X_test, y_test, display_labels=target_names, xticks_rotation=\"vertical\"\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "A02JsCcN34vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: train a logistic regression on the data projected on the principal components\n",
        "# What do you see?\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = None\n",
        "classification_report_and_plot(y_test, X_test, target_names, lr)"
      ],
      "metadata": {
        "id": "UWzV4epn3y6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: train a neural network classifier (MLP)\n",
        "# use different activation functions. What do you see?\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# TODO: try relu and logistic activations?\n",
        "mlp = None\n",
        "classification_report_and_plot(y_test, X_test, target_names, mlp)"
      ],
      "metadata": {
        "id": "-O2irBuM5yiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: plot predictions with their labels\n",
        "\n",
        "# Randomly pick 12 images to predict and display\n",
        "idx = np.random.choice(len(y_test), 12)\n",
        "images = X_test[idx]\n",
        "true_labels = [str(target_names[label]) for label in y_test[idx]]"
      ],
      "metadata": {
        "id": "bKblrwhK5Elb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hpdj5UPZhBkP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}