{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfIFFdrcQsSoKAJdgI0pfY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomascerbelaud/M2-IPE-data-energy/blob/main/notebooks/5_3_Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 5.3: Gradient descent"
      ],
      "metadata": {
        "id": "NjOvJdFjtGHn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79659dde"
      },
      "source": [
        "Demonstrate gradient descent by generating synthetic linear data, implementing the algorithm with a cost function, and visualizing its convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac6cee2a"
      },
      "source": [
        "## Data Generation\n",
        "\n",
        "Generate synthetic linear data (e.g., `y = mx + b + noise`) to demonstrate the gradient descent algorithm for a simple linear regression problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b8f10a1"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 2. Define the number of data points\n",
        "n_samples = 100\n",
        "\n",
        "# 3. Generate a set of 'x' values\n",
        "X = 2 * np.random.randn(n_samples, 1)\n",
        "\n",
        "# 4. Choose a true slope (m_true) and intercept (b_true)\n",
        "m_true = 3\n",
        "b_true = 2\n",
        "\n",
        "# 5. Calculate the 'y' values based on the formula y = m_true * x + b_true\n",
        "y = m_true * X + b_true\n",
        "\n",
        "# 6. Generate random noise and add it to the 'y' values\n",
        "noise = np.random.randn(n_samples, 1) * 2.0\n",
        "y = y + noise\n",
        "\n",
        "print(f\"Generated {n_samples} data points.\")\n",
        "print(f\"First 5 X values:\\n{X[:5].flatten()}\")\n",
        "print(f\"First 5 y values:\\n{y[:5].flatten()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92248361"
      },
      "source": [
        "## Training\n",
        "\n",
        "Implement the gradient descent algorithm, including defining a cost function (e.g., Mean Squared Error) and updating model parameters (slope and intercept) iteratively. Train using gradient descent and stochastic (mini batch) gradient descent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca381e41"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 2. Define the learning rate and the number of iterations\n",
        "learning_rate = 0.01\n",
        "n_iterations = 1000\n",
        "\n",
        "\n",
        "# 3. TODO: Implement the mean squared error (MSE) cost function\n",
        "def mse_cost(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def train(X: np.ndarray, y: np.ndarray, n_iterations: int, learning_rate: float, batch_size: int | None = None) -> tuple[list[float], list[float]]:\n",
        "  \"\"\"Implement full or stochastic gradient descent.\"\"\"\n",
        "\n",
        "  # Create lists to store the cost, m, and b at each iteration\n",
        "  cost_history = []\n",
        "  m_history = []\n",
        "  b_history = []\n",
        "\n",
        "  # 1. Initialize model parameters (slope 'm' and intercept 'b') with random values\n",
        "  m = np.random.randn(1)\n",
        "  b = np.random.randn(1)\n",
        "  print(f\"Initial m: {m[0]:.4f}, Initial b: {b[0]:.4f}\")\n",
        "\n",
        "  n_samples = batch_size or X.shape[0]\n",
        "\n",
        "  # Implement the gradient descent loop\n",
        "  for iteration in range(n_iterations):\n",
        "      # a. Calculate the predicted y_pred\n",
        "      if batch_size is not None:\n",
        "        batch_idx = np.random.choice(X.shape[0], size=batch_size)\n",
        "        X_batch = X[batch_idx]\n",
        "        y_batch = y[batch_idx]\n",
        "      else:\n",
        "        X_batch = X\n",
        "        y_batch = y\n",
        "\n",
        "      y_pred = m * X_batch + b\n",
        "\n",
        "      # b. Calculate the gradients for m and b\n",
        "      # Gradient of MSE with respect to m: (2/n) * sum((y_pred - y) * X)\n",
        "      # Gradient of MSE with respect to b: (2/n) * sum(y_pred - y)\n",
        "      gradients_m = (2/n_samples) * np.sum((y_pred - y_batch) * X_batch)\n",
        "      # TODO: compute gradients_b\n",
        "      gradients_b = 0.0\n",
        "\n",
        "      # c. Update m and b using the calculated gradients and the learning rate\n",
        "      m = m - learning_rate * gradients_m\n",
        "      # TODO: update b\n",
        "      b = 0.0\n",
        "\n",
        "      # d. Calculate the current cost (MSE) and store it, along with m and b\n",
        "      current_cost = mse_cost(y_pred, y_batch)\n",
        "      cost_history.append(current_cost)\n",
        "      m_history.append(m[0])\n",
        "      b_history.append(b[0])\n",
        "\n",
        "  # After the loop, print the final learned values of m and b, and the final cost\n",
        "  print(f\"Learned m: {m[0]:.4f}, Learned b: {b[0]:.4f}\")\n",
        "  print(f\"Final MSE cost: {cost_history[-1]:.4f}\")\n",
        "  print()\n",
        "\n",
        "  return (m_history, b_history), cost_history\n",
        "\n",
        "\n",
        "history, losses = train(X, y, n_iterations=n_iterations, learning_rate=learning_rate, batch_size=None)\n",
        "history_batch, losses_batch = train(X, y, n_iterations=n_iterations, learning_rate=learning_rate, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4b079bd"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Visualize the synthetic data, the initial random regression line, and the final learned regression line after applying gradient descent. Also, plot the cost function's convergence over iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f7afac0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with two subplots\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Subplot 1: Regression Lines\n",
        "plt.subplot(1, 2, 1) # 1 row, 2 columns, first plot\n",
        "\n",
        "# Create a scatter plot of the original synthetic data points\n",
        "plt.scatter(X, y, label='Synthetic Data', alpha=0.6)\n",
        "\n",
        "m_initial_plot = np.random.randn(1)\n",
        "b_initial_plot = np.random.randn(1)\n",
        "y_initial_pred = m_initial_plot * X + b_initial_plot\n",
        "\n",
        "# Plot the initial regression line (before gradient descent)\n",
        "plt.plot(X, y_initial_pred, color='red', linestyle='--', label=f'Initial Regression Line (m={m_initial_plot[0]:.2f}, b={b_initial_plot[0]:.2f})')\n",
        "\n",
        "# Plot the final learned regression line\n",
        "m, b = history[0][-1], history[1][-1]\n",
        "y_final_pred = m * X + b\n",
        "plt.plot(X, y_final_pred, color='green', linestyle='-', label=f'Final Regression Line (m={m:.2f}, b={b:.2f})')\n",
        "\n",
        "# Add appropriate labels and title\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Gradient Descent: Regression Line Convergence')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Subplot 2: Cost Function Convergence\n",
        "plt.subplot(1, 2, 2) # 1 row, 2 columns, second plot\n",
        "\n",
        "# Plot the cost_history over the n_iterations\n",
        "plt.plot(range(n_iterations), losses, alpha=.8, label=\"Gradient descent\")\n",
        "plt.plot(range(n_iterations), losses_batch, alpha=.8, label=\"Minibatch\")\n",
        "plt.legend()\n",
        "\n",
        "# Add appropriate labels and title\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.title('Gradient Descent: Cost Function Convergence')\n",
        "plt.grid(True)\n",
        "\n",
        "# Use plt.tight_layout() to prevent overlap and plt.show() to display the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_gd_trajectory(ax: Any, m_history: list[float], b_history: list[float], m_true: float, b_true: float):\n",
        "  # Define a grid for m and b values for visualization\n",
        "  # Range will be around the initial random values and the final learned values\n",
        "  m_initial = m_history[0]\n",
        "  b_initial = b_history[0]\n",
        "  m_bounds = m_initial, m_initial + 2 * (m - m_initial)\n",
        "  b_bounds = b_initial, b_initial + 2 * (b - b_initial)\n",
        "  m_min, m_max = min(m_bounds) - 0.5, max(m_bounds) + 0.5\n",
        "  b_min, b_max = min(b_bounds) - 0.5, max(b_bounds) + 0.5\n",
        "\n",
        "  m_grid = np.linspace(m_min, m_max, 100)\n",
        "  b_grid = np.linspace(b_min, b_max, 100)\n",
        "  M, B = np.meshgrid(m_grid, b_grid)\n",
        "\n",
        "  # Calculate the gradient norm for each point in the grid\n",
        "  Z_grad_norm = np.zeros(M.shape)\n",
        "  Z_mse_cost = np.zeros(M.shape)\n",
        "\n",
        "  for i in range(M.shape[0]):\n",
        "      for j in range(M.shape[1]):\n",
        "          m_val = M[i, j]\n",
        "          b_val = B[i, j]\n",
        "\n",
        "          y_pred_grid = m_val * X + b_val\n",
        "          gradients_m_grid = (2/n_samples) * np.sum((y_pred_grid - y) * X)\n",
        "          gradients_b_grid = (2/n_samples) * np.sum(y_pred_grid - y)\n",
        "          Z_grad_norm[i, j] = np.sqrt(gradients_m_grid**2 + gradients_b_grid**2)\n",
        "          Z_mse_cost[i, j] = mse_cost(y_pred_grid, y)\n",
        "\n",
        "  # Create the contour plot of the gradient norm\n",
        "  # contour = plt.contourf(M, B, Z_grad_norm, levels=50, cmap='viridis', alpha=0.8)\n",
        "  contour = ax.contourf(M, B, Z_mse_cost, levels=50, cmap='viridis', alpha=0.8)\n",
        "  plt.colorbar(contour, label='Gradient Norm')\n",
        "\n",
        "  # Plot the path taken by gradient descent\n",
        "  ax.plot(m_history, b_history, color='red', linestyle='-.', label='GD Path')\n",
        "\n",
        "  # Mark the start and end points\n",
        "  ax.scatter(m_history[0], b_history[0], color='blue', marker='o', s=100, label='Start')\n",
        "  ax.scatter(m_history[-1], b_history[-1], color='cyan', marker='x', s=100, label='End')\n",
        "\n",
        "  # Mark the true values\n",
        "  ax.scatter(m_true, b_true, color='magenta', marker='*', s=200, label='True (m_true, b_true)')\n",
        "\n",
        "  ax.set_xlabel('Slope (m)')\n",
        "  ax.set_ylabel('Intercept (b)')\n",
        "  ax.legend()\n",
        "  ax.grid(True)\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "plot_gd_trajectory(ax1, history[0], history[1], m_true, b_true)\n",
        "plot_gd_trajectory(ax2, history_batch[0], history_batch[1], m_true, b_true)\n",
        "ax1.set_title('Gradient Descent: Trajectory Visualization')\n",
        "ax2.set_title('Stochastic Gradient Descent: Cost Function Visualization')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lOGPtcxo-kZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nuBN9STVtlrj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}